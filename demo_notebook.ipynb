{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent Creator Demo Notebook ðŸ¤–\n",
        "\n",
        "This notebook demonstrates the key features of the Agent Creator library, including:\n",
        "- Research Agent capabilities\n",
        "- Webscraper Agent functionality\n",
        "- Integration between agents\n",
        "- MLX-powered AI processing\n",
        "\n",
        "Let's explore how to use these powerful tools for AI-powered research and web scraping!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:MLX not available. Using fallback mode.\n",
            "WARNING:root:DuckDuckGo Search not available\n",
            "WARNING:root:PDF generation dependencies not available\n",
            "WARNING:root:Selenium not available\n",
            "WARNING:root:Fake UserAgent not available\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "# Add the parent directory to Python path to import agent_creator\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd())))\n",
        "\n",
        "# Import our agents and utilities\n",
        "from agent_creator import ResearchAgent, WebscraperAgent\n",
        "from agent_creator.core.base_agent import AgentConfig, AgentTask, BaseAgent\n",
        "from agent_creator.agents.webscraper_agent import ScrapingConfig, ScrapingResult\n",
        "from agent_creator.agents.research_agent import ResearchResult\n",
        "from agent_creator.utils.llm_interface import LLMInterface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Initialize Research Agent\n",
        "\n",
        "First, let's set up our Research Agent with appropriate configuration. \n",
        "This agent will be responsible for conducting AI-powered research tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure and initialize the Research Agent\n",
        "research_config = AgentConfig(\n",
        "    name=\"DemoResearchAgent\",\n",
        "    description=\"AI-powered research agent for comprehensive online research\",\n",
        "    capabilities=[\n",
        "        \"web_search\", \"content_analysis\", \"citation_generation\",\n",
        "        \"pdf_generation\", \"notebook_generation\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "research_agent = ResearchAgent(research_config)\n",
        "research_agent.start()  # Start the agent\n",
        "\n",
        "print(\"Research Agent initialized with capabilities:\", research_agent.config.capabilities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Initialize Webscraper Agent\n",
        "\n",
        "Now, let's set up our Webscraper Agent with appropriate configuration for content extraction and web scraping tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure and initialize the Webscraper Agent\n",
        "webscraper_config = AgentConfig(\n",
        "    name=\"DemoWebscraperAgent\",\n",
        "    description=\"Advanced web scraping agent for content extraction\",\n",
        "    capabilities=[\n",
        "        \"url_scraping\", \"content_extraction\", \"link_extraction\",\n",
        "        \"image_extraction\", \"metadata_extraction\", \"batch_scraping\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Set up scraping-specific configuration\n",
        "scraping_config = ScrapingConfig(\n",
        "    timeout=30,\n",
        "    delay_between_requests=1.0,\n",
        "    max_content_length=2000000  # 2MB\n",
        ")\n",
        "\n",
        "webscraper_agent = WebscraperAgent(webscraper_config, scraping_config)\n",
        "webscraper_agent.start()  # Start the agent\n",
        "\n",
        "print(\"Webscraper Agent initialized with capabilities:\", webscraper_agent.config.capabilities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Connect Agents\n",
        "\n",
        "Let's connect our Research Agent with the Webscraper Agent to enable seamless collaboration between them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect the Webscraper Agent to the Research Agent\n",
        "research_agent.set_webscraper_agent(webscraper_agent)\n",
        "\n",
        "print(\"Agents connected successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Demo: Research Task\n",
        "\n",
        "Let's demonstrate how to use the Research Agent to conduct research on a specific topic. This will showcase the agent's ability to search, analyze, and generate insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example research task\n",
        "research_query = \"Latest developments in MLX and machine learning on Apple Silicon\"\n",
        "\n",
        "# Create research task\n",
        "research_task_id = research_agent.create_task(\n",
        "    description=\"Research MLX and Apple Silicon developments\",\n",
        "    parameters={\n",
        "        \"type\": \"research\",\n",
        "        \"query\": research_query,\n",
        "        \"max_results\": 5\n",
        "    }\n",
        ")\n",
        "\n",
        "# Execute research task\n",
        "research_result: ResearchResult = research_agent.run_task(research_task_id)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nResearch Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Query: {research_result.query}\")\n",
        "print(f\"\\nSummary:\\n{research_result.summary}\\n\")\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for idx, source in enumerate(research_result.sources, 1):\n",
        "    print(f\"\\nSource {idx}:\")\n",
        "    print(f\"Title: {source['title']}\")\n",
        "    print(f\"URL: {source['url']}\")\n",
        "    print(f\"Snippet: {source['snippet'][:200]}...\")\n",
        "\n",
        "print(\"\\nCitations:\")\n",
        "for citation in research_result.citations:\n",
        "    print(f\"- {citation}\")\n",
        "\n",
        "# Generate PDF report\n",
        "pdf_task_id = research_agent.create_task(\n",
        "    description=\"Generate PDF report from research results\",\n",
        "    parameters={\n",
        "        \"type\": \"generate_pdf\",\n",
        "        \"research_result\": research_result.__dict__,\n",
        "        \"filename\": \"mlx_research_report.pdf\"\n",
        "    }\n",
        ")\n",
        "pdf_path = research_agent.run_task(pdf_task_id)\n",
        "print(f\"\\nPDF report generated: {pdf_path}\")\n",
        "\n",
        "# Generate analysis notebook\n",
        "notebook_task_id = research_agent.create_task(\n",
        "    description=\"Generate analysis notebook from research results\",\n",
        "    parameters={\n",
        "        \"type\": \"generate_notebook\",\n",
        "        \"research_result\": research_result.__dict__,\n",
        "        \"filename\": \"mlx_research_analysis.ipynb\"\n",
        "    }\n",
        ")\n",
        "notebook_path = research_agent.run_task(notebook_task_id)\n",
        "print(f\"Analysis notebook generated: {notebook_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Demo: Web Scraping Task\n",
        "\n",
        "Now let's demonstrate the Webscraper Agent's capabilities by extracting content from a specific URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example web scraping task\n",
        "target_url = \"https://ml-explore.github.io/mlx/build/html/index.html\"\n",
        "\n",
        "# Create scraping task\n",
        "scraping_task_id = webscraper_agent.create_task(\n",
        "    description=\"Scrape MLX documentation page\",\n",
        "    parameters={\n",
        "        \"type\": \"scrape_url\",\n",
        "        \"url\": target_url,\n",
        "        \"use_selenium\": False  # Use requests by default\n",
        "    }\n",
        ")\n",
        "\n",
        "# Execute scraping task\n",
        "scraping_result: ScrapingResult = webscraper_agent.run_task(scraping_task_id)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nWeb Scraping Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"URL: {scraping_result.url}\")\n",
        "print(f\"Success: {scraping_result.success}\")\n",
        "print(f\"\\nTitle: {scraping_result.title}\")\n",
        "print(f\"Content Preview: {scraping_result.text[:200]}...\")\n",
        "\n",
        "# Display extracted links (if any)\n",
        "if scraping_result.links:\n",
        "    print(\"\\nExtracted Links (first 5):\")\n",
        "    for link in scraping_result.links[:5]:\n",
        "        print(f\"- {link}\")\n",
        "\n",
        "# Display extracted images (if any)\n",
        "if scraping_result.images:\n",
        "    print(\"\\nExtracted Images (first 3):\")\n",
        "    for image in scraping_result.images[:3]:\n",
        "        print(f\"- {image}\")\n",
        "\n",
        "# Display metadata\n",
        "print(\"\\nMetadata:\")\n",
        "for key, value in scraping_result.metadata.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "if scraping_result.error:\n",
        "    print(f\"\\nError: {scraping_result.error}\")\n",
        "\n",
        "print(f\"\\nResponse Time: {scraping_result.response_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Error Handling and Cleanup\n",
        "\n",
        "Finally, let's demonstrate proper error handling and cleanup of our agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Example of handling potential errors\n",
        "    invalid_url = \"https://nonexistent-website-12345.com\"\n",
        "    error_task_id = webscraper_agent.create_task(\n",
        "        description=\"Attempt to scrape non-existent website\",\n",
        "        parameters={\n",
        "            \"type\": \"scrape_url\",\n",
        "            \"url\": invalid_url\n",
        "        }\n",
        "    )\n",
        "    result: ScrapingResult = webscraper_agent.run_task(error_task_id)\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"Expected error occurred: {result.error}\")\n",
        "    else:\n",
        "        print(\"Unexpected success!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error occurred: {str(e)}\")\n",
        "finally:\n",
        "    # Get final task statuses\n",
        "    print(\"\\nFinal Task Statuses:\")\n",
        "    print(\"-\" * 50)\n",
        "    for task_id in [research_task_id, scraping_task_id, error_task_id]:\n",
        "        status = webscraper_agent.get_task_status(task_id)\n",
        "        print(f\"Task {status['task_id']}: {status['status']}\")\n",
        "    \n",
        "    # Cleanup\n",
        "    research_agent.stop()\n",
        "    webscraper_agent.stop()\n",
        "    print(\"\\nAgents cleaned up successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
